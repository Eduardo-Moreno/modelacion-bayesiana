
# Regresión lineal Bayesiana

Tenemos un conjunto de atributos o predictores $x \in \mathbb{R}^p$ y esperamos poder utilizarlos para inferir una respuesta $y \in \mathbb{R}$. Intrínsecamente, suponemos que existe une relación entre $y$ y $x$:


$$ y = h(x) + \varepsilon $$


En donde $\varepsilon$ representa todos aquellos elementos qeu no son representados en la función $h$.

Trabajaremos bajo un enfoque $100\%$ bayesiano:
+ Tenemos datos: $(x,y)$
+ Tenemos parámetros: $(h,\varepsilon)$

Supondremos lo siguiente:
$$ h \in \{ h_{\theta}:\theta \in \mathbb{R}^q \} $$
Ejemplo:
+ Modelo lineal:

$$ h(x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p$$


Luego,


$$ y = h(x) + \varepsilon; \quad \varepsilon \sim N(0, \sigma^2) $$


Definamos las siguientes variables de apoyo:
+ $\beta = (\beta_0, \beta_1, ... ,\beta_p) \in \mathbb{R}^q;q = p+1$
+ $X = (1, x_1, x_2, ..., x_p) \in \mathbb{R}^{n \text{x} q}$

Por lo que la reformulación del problema es:
1) Verosimilitud: $y|x, \beta, \sigma^2 \sim N(X\beta, \sigma^2)$
2) Distribución previa: $\beta, \sigma^2 \sim \pi(\beta, \sigma^2)$

Así pues, si tenemos $n \in \mathbb{N}$ observaciones $(x^{(i)}, y^{(i)})_{i=1}^n$, entonces la función de verosimilitud toma la siguiente forma:

$$ \Pi( \underline{y}_n | \underline{x}_n, \beta, \sigma^2 ) = \Pi_{i=1}^n \pi(y^{(i)}|x^{(i)}, \beta, \sigma^2) $$

Finalmente reformulando el problema obtenemos lo siguiente:
$$ Y = X\beta + \varepsilon, \quad \varepsilon \sim N(0, \Sigma) : \Sigma = \sigma^2\mathbb{I} $$

Por lo tanto:
1) Verosimilitud: $Y|X,\beta,\Sigma \sim N(XB, \Sigma)$
2) Previa: $\beta, \Sigma \sim \pi(\beta, \Sigma)$

Por lo que tenemos los ingredientes necesarios para constriuir una distribución posterior pra $\beta, \Sigma$.

* Caso 1 $\beta \sim N(\beta_0, \Simga_0)$

Suponemos que 
$$ \Sigma = \sigma^2\mathbb{I}  $$
Esto quiere decir que las covariables no tienen covarianza, en este caso particular (distribución normal) que las covariables son independientes. Luego,

$$
\Pi(\beta |Y,X) \propto \Pi(Y|X\beta) \Pi(\beta) \propto \text{exp} \Big (-\frac{1}{2} (X\beta-Y)^T(X\beta-Y) \Big)\text{exp} \Big(-\frac{1}{2}(\beta-\beta_0)^T \Sigma_n^{-1} (\beta-\beta_0) \Big)

\\

\therefore \beta|X,Y \sim N(\beta_n, \Sigma_m)
$$

En donde:
$$
\Sigma_n^{-1} = \Big( \frac{X^TX}{\sigma^2} + \Sigma_0^{-1} \Big)
$$

Recordemos que del problema de regresión lineal analizado en el curso de Fundamentos de Estadística obtenemos lo siguiente:

$$
\mathbb{V}(\hat{\beta}) = \sigma^2(X^TX)^{-1} \Leftrightarrow \mathbb{V}(\hat{\beta})^{-1} = \sigma^{-2}(X^TX) = \frac{(X^TX)}{\sigma^2} 
$$

Definimos:

$$
\mathbb{V}(\hat{\beta}) = \Sigma_{\hat{\beta}}
$$

Entonces:

$$
\Sigma_n^{-1} = \Big( \Sigma_{\hat{\beta}} + \Sigma_0^{-1} \Big)
$$

Además:

$$
\beta_n = \Sigma_n(\Sigma_{\hat{\beta}} \hat{\beta} + \Sigma_0 \beta_0 )

\\

\hat{\beta} = (XX^T)^{-1}X^TY
$$

De donde analizamos los siguientes casos:


+ Si $\Sigma_0$ tiene valores propios grandes, entonces eso quiere decir que tenemos información muy vaga. Luego:

$$
\Sigma_0 \approx 0 \quad \therefore \quad \Sigma_n = \Sigma_{\hat{\beta}}, \quad \beta_n = \hat{\beta}
$$
+ Si $\Sigma_0$ tiene valores propios pequeños, esto quiere decir que tenemos información muy precisa. Luego:

$$
\Sigma^{-1}_0 >> \Sigma_{\hat{\beta}}^{-1}, \quad \Sigma_n^{-1} \approx \Sigma_0^{-1}, \quad \beta_n \approx \beta_0
$$


```{r, include=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(cmdstanr)
library(rstanarm)
library(bayesplot)

library(patchwork)
library(scales)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, 
                      fig.align = 'center', fig.width = 5, fig.height=3, cache = TRUE)
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_linedraw())
color.blues <- c(NA,"#BDD7E7", "#6BAED6", "#3182BD", "#08519C", "#074789", "#063e77", "#053464")
color.itam  <- c("#00362b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f")


sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
sin_leyenda <- theme(legend.position = "none")
sin_ejes <- theme(axis.ticks = element_blank(), 
                  axis.text = element_blank())

source("../funciones-auxiliares/setup-utility.R")
```

Se tienen datos de candidatos que se reeligieron, gobernante y tasa de crecimiento económico.  

```{r }
hibbs <- read.table("../datos/hibbs.dat", header=TRUE)
hibbs <- hibbs %>% 
    mutate(Crecimiento = cut(growth, 
                        breaks = c(-Inf, 0, 1, 2, 3, 4, Inf), 
                        labels = c("Negativo", "0 - 1%", "1 - 2%", 
                                   "2 - 3%", "3 - 4%", 
                                   "Arriba de 4%")),
           Crecimiento = fct_rev(Crecimiento))
```

Se quiere predecir el porcentaje de votos en relación al crecimiento económico obtenido en el periodo de gobierno. La recta punteada indica si el candidato en cuestión continuará en el gobierno.

```{r }

gdatos <- hibbs %>% 
    ggplot(aes(vote, growth, color = Crecimiento)) + 
        geom_point() + 
        geom_vline(xintercept = 50, lty = 2) + 
        xlab("Porcentaje de voto a favor (%)") + 
        ylab("Crecimiento Económico (%)") + 
        coord_flip() + sin_lineas

gdatos

```

Modelo en STAN con 2 parámetros $(a,b)$. El modelo es lineal de la forma:

$$
y = b + ax
$$

```{r}
ruta <- "modelos-stan/regesion-previa.stan"
print_file(ruta)
```

Se compila el modelo

```{r}
modelo.previa <- cmdstan_model(ruta)
```

Se muestrea del modelo

```{r}
muestras <- modelo.previa$sample(data = list(sigma = 2), 
                          chains = 1, 
                          iter=200, 
                          iter_warmup=500, 
                          seed=483892929, 
                          refresh=1200)
```

Impresión del resumen del modelo, el cual son simulaciones de un modelo a priori.

```{r}
muestras$cmdstan_summary()
```


```{r}
muestras.previa <- tibble(posterior::as_draws_df(muestras$draws(c("a","b"))))
head(muestras.previa)
```

Del lado derecho se grafica todas un conjunto de los modelo posibles obtenidos con las simulaciones realizadas. Nótese que existen pendientes negativas, lo cual no debe alarmarnos, pues son meras simulaciones y aún no se incorporan los datos.

```{r}

calcula_linea <- function(data){
    tibble(x =  seq(-1, 5, by = 1/100), 
           y = x * data$a + data$b)
}

gprevia <- muestras.previa %>% 
    nest(a,b) %>% 
    mutate(lines = map(data, calcula_linea)) %>% 
    unnest(lines) %>% 
    ggplot(aes(x, y, group = interaction(.chain, .iteration))) + 
        geom_line(alpha = .2) + sin_lineas + 
        geom_hline(yintercept = 50, lty = 2) +
        ylim(44, 62)

gdatos + gprevia

```

Ahora se estimará el modelo posterior, para ello se necesitarán de los datos.

```{r}
ruta <- "modelos-stan/regesion-posterior.stan"
print_file(ruta)
```

Se compila el modelo

```{r}
modelo.posterior <- cmdstan_model(ruta)
```

Se obtienen simulaciones del modelo.

```{r}
data_list <- list(sigma_0 = 2, N = nrow(hibbs), y = hibbs$vote, x = hibbs$growth)

muestras <- modelo.posterior$sample(data = data_list,
                          chains = 1, 
                          iter=500, 
                          iter_warmup=500, 
                          seed=483892929, 
                          refresh=1200)
```

```{r}
muestras$cmdstan_summary()
```


Ahora se muestran las rectas en base al modelo posterior. Se logra observar que se ha aprendido en base a los datos.

```{r, fig.asp = .3}
muestras.posterior <- tibble(posterior::as_draws_df(muestras$draws(c("a","b", "sigma"))))

gposterior <- muestras.posterior %>% 
    nest(a,b) %>% 
    sample_frac(.2) %>% 
    mutate(lines = map(data, calcula_linea)) %>% 
    unnest(lines) %>% 
    ggplot(aes(x, y, group = interaction(.chain, .iteration))) + 
        geom_line(alpha = .2) + sin_lineas + 
        geom_hline(yintercept = 50, lty = 2) + 
        geom_point(data = hibbs %>% mutate(.chain = 1, .iteration = 1), 
                   aes(x = growth, y = vote)) + 
        ylim(44, 62) 

gdatos + gprevia + gposterior

```


Resumen del modelo, se estima los errores del componente del error (desviación respecto a la recta)

```{r}
muestras.posterior %>% 
    pivot_longer(cols = a:sigma, names_to = "parameter") %>% 
    group_by(parameter) %>% 
    summarise(mean = mean(value), var = var(value), std = sd(value), .groups = "drop")
```

Se comparará con el modelo obtenido en base a la metodología de máxima verosimilitud.

```{r }

model <- lm(vote ~ growth, hibbs)
summary(model)

```

### Predicciones con `Stan` {-}

Si se supone una tasa de crecimiento dada, ¿cuál es la probabilidad de que dicho presidente gane las siguientes elecciones?

```{r}
ruta <- "modelos-stan/regresion-prediccion.stan"
print_file(ruta)
```

```{r}
modelo.prediccion <- cmdstan_model(ruta)
```

```{r}
data_list <- list(sigma_0 = 2, N = nrow(hibbs), y = hibbs$vote, x = hibbs$growth)

muestras <- modelo.prediccion$sample(data = data_list,
                          chains = 1, 
                          iter=500, 
                          iter_warmup=500, 
                          seed=483892929, 
                          refresh=1200)
```

```{r}
muestras$cmdstan_summary()
```

```{r}
muestras.prediccion <- tibble(posterior::as_draws_df(muestras$draws(c("y_new"))))

muestras.prediccion %>% 
    ggplot(aes(x = y_new)) + 
        geom_histogram(binwidth = 2, alpha = .8) + 
        geom_vline(xintercept = 50, lty = 2) + 
        xlab("Porcentaje de voto a favor") + sin_lineas

muestras.prediccion %>% 
    summarise(cuantiles = quantile(y_new, probs = c(.05, .50, .95)))

muestras.prediccion %>% 
    mutate(wins = y_new > 50) %>% 
    summarise(`P(ganar)` = mean(wins))

```


```{r}

model <- lm(vote ~ growth, data = hibbs)
summary(model)
predict(model, newdata = tibble(growth = 2), interval = "confidence")

```

En base al modelo obtenido con los datos que se cuenta, si se tiene un crecimiento económico del $2\%$ entonces la probabilidad media de que el presidente gane las siguientes elecciones es del $52\%$. Además, se cuenta con el intervalo de densidad para dicha probabilidad $[50\%, 54\%]$ del $95\%$. El intervalo del modelo bayesiano posee valores mayores, debido a que, incorpora mayor incertidumbre en los parámetros $(\sigma^2)$.


### Usando `rstanarm::stan_glm` {-}

Para no volver a correr el mismo modelo en STAN cada vez que se quiera utilizar se utiliza la siguiente paquetería `rstanarm` desarrollada por varios estadísticos, que nos ayuda a ajustar modelos de regresión de manera similar a modelos no bayesianos (se asume que el lector está familiarizado con dicha notación usual).

```{r }
M1 <- stan_glm(vote ~ growth, data = hibbs, refresh = 0)
```

Print default summary of the fitted model

```{r }
print(M1)
```


Información sobre la distribución previa utilizada para el ajuste del modelo bayesiano obtenido anteriormente. Cabe mencionar que en la función `stan_glm` los valores por default para el modelo de regresión lineal:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + ... + \beta_p x_p + \varepsilon
$$

Son:
+ $\beta_k \sim N \Big (0, (2.5)^2 \frac{\hat{\sigma}_y^2}{\hat{\sigma}_{x_k}^2} \Big) \quad \forall k=1,2,...,p$
+ $\beta_0 \sim N \Big( \bar{y}_n, (2.5)^2 \hat{\sigma}_y^2 \Big)$
+ $\sigma_{\varepsilon} \sim \text{Exponential} \Big ( 1/\hat{\sigma_y}^2 \Big)$

En donde,
$$\hat{\sigma_y}^2 = \frac{1}{n-1}\sum_{i=1}^n (y^{(i)}-\bar{y}_n)^2 $$

```{r }
prior_summary(M1)
```


```{r }
summary(M1)
```


```{r }
round(posterior_interval(M1),1)
```


```{r }

ggplot(data = hibbs, aes(x = growth, y = vote))  + 
    geom_point() + 
    geom_abline(slope = coef(M1)[2], intercept = coef(M1)[1], color = 'salmon') + 
    sin_lineas

```


```{r}

ggplot(data = hibbs, aes(x = growth, y = vote))  + 
    geom_abline(data = as_tibble(M1) %>% sample_frac(.2), 
                aes(slope = growth, intercept = `(Intercept)`), 
                color = 'grey', alpha = .4) + 
    geom_abline(slope = coef(M1)[2], intercept = coef(M1)[1], color = 'salmon') + 
    geom_point() + 
    sin_lineas

```

### Predicciones usando `stan_glm`

Predicciones puntuales en el porcentaje de votos a favor dado un crecimiento económico del $2\%$

```{r }
new <- data.frame(growth=2.0)
y_point_pred <- predict(M1, newdata=new)
y_point_pred
```

### Predicciones con incertidumbre {-}

```{r }
y_linpred <- posterior_linpred(M1, newdata=new)
summary(y_linpred)
```

```{r }
y_pred <- posterior_predict(M1, newdata=new)
summary(y_pred)
```

## Incorporando estudios previos {-}

Ahora, analicemos el caso en donde se analizó a $3,000$ familias distintas y se registró si el padre era visualmente atractivo o no en una escala discreta $(-2,-1,0,1,2)$ y además si el bebé es niño o niña. Los datos ya se encuentran agrupados, de tal manera que se muestra la proporción de niñas en base al nivel de atracción (visual-física) del padre.

```{r}

x <- seq(-2,2,1)
y <- c(50, 44, 50, 47, 56)
sexratio <- data.frame(x, y)

ratios <- sexratio %>% 
    mutate(groups = ifelse(x < 2, 0, 1))

summary(lm(y ~ groups, ratios))

```

Los papás más atractivos, tienen el $56\%$ de las niñas.
Suponiendo que el modelo es correcto, se tiene evidencia estadística para indicar que la relación se mantiene y se podría concluir que los papás atractivos tienen tendencia a tener niñas.

Ahora, utilicemos modelos bayesianos para reproducir el ejercicio anterior.

```{r}

theta_hat_prior <- 0
se_prior <- 0.25
theta_hat_data <- 8
se_data <- 3
theta_hat_bayes <- (theta_hat_prior/se_prior^2 + theta_hat_data/se_data^2)/(1/se_prior^2 + 1/se_data^2)
se_bayes <- sqrt(1/(1/se_prior^2 + 1/se_data^2))

```


```{r}

gols <- sexratio %>% 
    ggplot(aes(x, y)) + 
        geom_point() + 
        geom_smooth(method = 'lm', color = 'salmon') +
        xlab('Belleza en padres') + ylab("Porcentaje de niñas") + 
        ggtitle("Solución de mínimos cuadrados") + sin_lineas

gols
```

Nótese que el nivel de incertidumbre es muy alta.

```{r}

model.ls <- lm(y ~ x, sexratio)
summary(model.ls)

```

Utilizando `stan_glm`

```{r}
fit_ols <- stan_glm(y ~ x, data = sexratio, refresh = 0, 
                        prior = NULL, prior_intercept = NULL, prior_aux = NULL)

fit_default <- stan_glm(y ~ x, data = sexratio, refresh = 0)

print(fit_default)

```

Incorporemos la información con la que se contaba anteriormente, se calcula el modelo y se muestran las diferencias entre todos los modelos.

```{r}

fit_post <- stan_glm(y ~ x, data = sexratio,
                     prior = normal(0, 0.2),
                     prior_intercept = normal(48.8, 0.5),
                     refresh = 0)
print(fit_post)

```


```{r, fig.asp = .4}

gdefault <- ggplot(data = sexratio, aes(x = x, y = y))  + 
    geom_abline(data = as_tibble(fit_default) %>% sample_frac(.1),
                aes(slope = x, intercept = `(Intercept)`),
                color = 'grey', alpha = .4) +
    geom_abline(slope = coef(fit_default)[2], intercept = coef(fit_default)[1], color = 'salmon') +
    geom_point() + ggtitle("Ajuste con default") + ylim(35,65) + ylab("") + 
    sin_lineas

gposterior <- ggplot(data = sexratio, aes(x = x, y = y))  + 
    geom_abline(data = as_tibble(fit_post) %>% sample_frac(.1),
                aes(slope = x, intercept = `(Intercept)`),
                color = 'grey', alpha = .4) +
    geom_abline(slope = coef(fit_post)[2], intercept = coef(fit_post)[1], color = 'salmon') +
    geom_hline(yintercept = 48.8, lty = 2) + 
    geom_point() + ggtitle("Ajuste posterior") + ylim(35,65) +  ylab("") + 
    sin_lineas

gols + gdefault + gposterior

```

+ El modelo del lado izquierdo: frecuentista obtenido con mínimos cuadrados, nótese que presenta una gran dispersión.
+ En medio: modelo por default.
+ El modelo del lado derecho: se ajusta por la información y se llega a una conclusión contradictoria. Se muestra el potencial de incorporar información útil dentro del contexto del modelo para obtener resultados coherentes.
